{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZG6gFaqeH2D8Z1XprX2Bm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navin2065/Text-Summarization/blob/main/Another_copy_of_Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy nltk scikit-learn networkx gradio pdfminer.six\n",
        "\n",
        "# Download necessary models\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydz-u9b8k16X",
        "outputId": "277565ea-4cc4-445b-bd51-df990ebd2c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to preprocess and tokenize sentences\n",
        "def preprocess_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    cleaned = [sent.strip().replace(\"\\n\", \" \") for sent in sentences if len(sent.strip()) > 0]\n",
        "    return cleaned\n",
        "\n",
        "# Function to compute sentence embeddings using spaCy\n",
        "def sentence_embeddings(sentences):\n",
        "    return [nlp(sentence).vector for sentence in sentences]\n",
        "\n",
        "# Build similarity matrix between sentence embeddings\n",
        "def build_similarity_matrix(embeddings):\n",
        "    sim_matrix = np.zeros((len(embeddings), len(embeddings)))\n",
        "    for i in range(len(embeddings)):\n",
        "        for j in range(len(embeddings)):\n",
        "            if i != j:\n",
        "                sim_matrix[i][j] = cosine_similarity(\n",
        "                    embeddings[i].reshape(1, -1),\n",
        "                    embeddings[j].reshape(1, -1)\n",
        "                )[0, 0]\n",
        "    return sim_matrix\n",
        "\n",
        "# Summarizer using TextRank\n",
        "def summarize_text(text, num_sentences=None):\n",
        "    sentences = preprocess_sentences(text)\n",
        "    if len(sentences) == 0:\n",
        "        return \"No valid sentences found.\"\n",
        "\n",
        "    embeddings = sentence_embeddings(sentences)\n",
        "    sim_matrix = build_similarity_matrix(embeddings)\n",
        "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    ranked_sentences = sorted(((scores[i], s, i) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    if num_sentences is None or num_sentences > len(sentences):\n",
        "        num_sentences = len(sentences)\n",
        "\n",
        "    selected = sorted(ranked_sentences[:num_sentences], key=lambda x: x[2])\n",
        "    return \" \".join([s[1] for s in selected])\n"
      ],
      "metadata": {
        "id": "1enGpbb1k5SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle file input (txt and pdf)\n",
        "def extract_text_from_file(file):\n",
        "    if file.name.endswith(\".txt\"):\n",
        "        # Read .txt file\n",
        "        text = file.read().decode(\"utf-8\")\n",
        "    elif file.name.endswith(\".pdf\"):\n",
        "        # Extract text from PDF\n",
        "        text = extract_text(file)\n",
        "    else:\n",
        "        text = None\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "gIVFnAI8k-NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Gradio interface function\n",
        "def gradio_summarizer(file, num_sentences):\n",
        "    # Extract text from the uploaded file\n",
        "    text = extract_text_from_file(file)\n",
        "\n",
        "    if not text:\n",
        "        return \"Invalid file format. Please upload a .txt or .pdf file.\"\n",
        "\n",
        "    # Check if the file is empty or contains invalid content\n",
        "    if len(text.strip()) == 0:\n",
        "        return \"The uploaded file is empty. Please upload a valid file.\"\n",
        "\n",
        "    # Summarize the extracted text\n",
        "    summary = summarize_text(text, num_sentences)\n",
        "    return summary\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_summarizer,\n",
        "    inputs=[\n",
        "        gr.File(file_count=\"single\", type=\"filepath\", label=\"Upload a Text or PDF File\"),  # File Upload: Changed type to \"filepath\"\n",
        "        gr.Slider(minimum=1, maximum=100, step=1, value=5, label=\"Number of Sentences to Summarize\")  # Number of sentences slider: Changed 'default' to 'value'\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Summary\", lines=15),  # Summary output with more lines for longer text\n",
        "    live=True  # Show summary in real-time\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "dohiy0rVmsD3",
        "outputId": "f5ddc077-2a3b-438a-d482-0d17315d8f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e87a29ee6486bfad25.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e87a29ee6486bfad25.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers gradio pymupdf\n",
        "\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Load summarization pipeline\n",
        "try:\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading summarization model:\", e)\n",
        "\n",
        "def extract_text_from_pdf(filepath):\n",
        "    try:\n",
        "        doc = fitz.open(filepath)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return None, f\"Error extracting PDF text: {e}\"\n",
        "\n",
        "def summarize_text(file):\n",
        "    if file is None:\n",
        "        return \"Please upload a file (.txt or .pdf).\"\n",
        "\n",
        "    filename = file.name.lower()\n",
        "\n",
        "    # Handle txt file\n",
        "    if filename.endswith(\".txt\"):\n",
        "        try:\n",
        "            with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            return \"Error decoding .txt file. Please upload a UTF-8 encoded text file.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error reading file: {e}\"\n",
        "\n",
        "    # Handle pdf file\n",
        "    elif filename.endswith(\".pdf\"):\n",
        "        text, error = extract_text_from_pdf(file.name)\n",
        "        if text is None:\n",
        "            return error\n",
        "    else:\n",
        "        return \"Unsupported file format. Please upload .txt or .pdf files only.\"\n",
        "\n",
        "    if len(text.strip()) == 0:\n",
        "        return \"Uploaded file is empty or no extractable text found.\"\n",
        "\n",
        "    max_len = 3000\n",
        "    if len(text) > max_len:\n",
        "        text = text[:max_len] + \"...\"\n",
        "\n",
        "    try:\n",
        "        summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        return f\"Error during summarization: {e}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=summarize_text,\n",
        "    inputs=gr.File(file_types=[\".txt\", \".pdf\"], label=\"Upload a text or PDF file\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Text & PDF Summarization Tool\",\n",
        "    description=\"Upload a .txt or .pdf file and get an NLP-generated summary.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "o_I_PUL27DTm",
        "outputId": "16e64273-a123-49b2-aab4-ed691930ec1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e50015a34488fb32ee.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e50015a34488fb32ee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers gradio pymupdf\n",
        "\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import fitz\n",
        "\n",
        "# Use a smaller, faster summarization model\n",
        "try:\n",
        "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading summarization model:\", e)\n",
        "\n",
        "def extract_text_from_pdf(filepath):\n",
        "    try:\n",
        "        doc = fitz.open(filepath)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return None, f\"Error extracting PDF text: {e}\"\n",
        "\n",
        "def chunk_text(text, max_words=600):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_words):\n",
        "        chunk = \" \".join(words[i:i+max_words])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def summarize_text(file, num_sentences):\n",
        "    if file is None:\n",
        "        return \"Please upload a file (.txt or .pdf).\"\n",
        "\n",
        "    filename = file.name.lower()\n",
        "\n",
        "    if filename.endswith(\".txt\"):\n",
        "        try:\n",
        "            with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "        except Exception as e:\n",
        "            return f\"Error reading file: {e}\"\n",
        "    elif filename.endswith(\".pdf\"):\n",
        "        text, error = extract_text_from_pdf(file.name)\n",
        "        if text is None:\n",
        "            return error\n",
        "    else:\n",
        "        return \"Unsupported file format. Please upload .txt or .pdf files only.\"\n",
        "\n",
        "    if not text.strip():\n",
        "        return \"Uploaded file is empty or no extractable text found.\"\n",
        "\n",
        "    # Limit text length to first 5000 characters for speed\n",
        "    text = text[:5000]\n",
        "\n",
        "    chunks = chunk_text(text, max_words=600)\n",
        "\n",
        "    # Limit number of chunks to 3 for speed\n",
        "    chunks = chunks[:3]\n",
        "\n",
        "    max_length_tokens = max(30, min(num_sentences * 20, 512))\n",
        "    min_length_tokens = max(5, max_length_tokens // 4)\n",
        "\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        try:\n",
        "            summary = summarizer(\n",
        "                chunk,\n",
        "                max_length=max_length_tokens,\n",
        "                min_length=min_length_tokens,\n",
        "                do_sample=False\n",
        "            )\n",
        "            summaries.append(summary[0]['summary_text'])\n",
        "        except Exception as e:\n",
        "            summaries.append(f\"[Error summarizing chunk: {e}]\")\n",
        "\n",
        "    # Join summaries without re-summarizing for speed\n",
        "    return \"\\n\\n\".join(summaries)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=summarize_text,\n",
        "    inputs=[\n",
        "        gr.File(file_types=[\".txt\", \".pdf\"], label=\"Upload a text or PDF file\"),\n",
        "        gr.Slider(1, 500, step=1, value=20, label=\"Number of sentences in summary (max 100 for speed)\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Fast Text & PDF Summarization Tool\",\n",
        "    description=\"Upload a .txt or .pdf file and choose number of sentences (up to 100) for a fast summary.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "1xwVJa6EAUUO",
        "outputId": "eba563b6-bfbe-4b9b-95e1-1b3c44ddda91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e1c2e1f1228411b3e6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e1c2e1f1228411b3e6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers gradio pymupdf python-docx\n",
        "\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import fitz  # PyMuPDF\n",
        "import docx\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "def extract_text_from_pdf(filepath):\n",
        "    try:\n",
        "        doc = fitz.open(filepath)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return None, f\"Error extracting PDF text: {e}\"\n",
        "\n",
        "def extract_text_from_docx(filepath):\n",
        "    try:\n",
        "        doc = docx.Document(filepath)\n",
        "        fullText = []\n",
        "        for para in doc.paragraphs:\n",
        "            fullText.append(para.text)\n",
        "        return \"\\n\".join(fullText)\n",
        "    except Exception as e:\n",
        "        return None, f\"Error extracting DOCX text: {e}\"\n",
        "\n",
        "def summarize_text(file, num_sentences):\n",
        "    if file is None:\n",
        "        return \"Please upload a file (.txt, .pdf or .docx).\"\n",
        "\n",
        "    filename = file.name.lower()\n",
        "\n",
        "    if filename.endswith(\".txt\"):\n",
        "        try:\n",
        "            with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            return \"Error decoding .txt file. Please upload a UTF-8 encoded text file.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error reading file: {e}\"\n",
        "\n",
        "    elif filename.endswith(\".pdf\"):\n",
        "        text, error = extract_text_from_pdf(file.name)\n",
        "        if text is None:\n",
        "            return error\n",
        "\n",
        "    elif filename.endswith(\".docx\"):\n",
        "        text, error = extract_text_from_docx(file.name)\n",
        "        if text is None:\n",
        "            return error\n",
        "    else:\n",
        "        return \"Unsupported file format. Please upload .txt, .pdf or .docx files only.\"\n",
        "\n",
        "    if len(text.strip()) == 0:\n",
        "        return \"Uploaded file is empty or no extractable text found.\"\n",
        "\n",
        "    max_chars = 3000\n",
        "    if len(text) > max_chars:\n",
        "        text = text[:max_chars] + \"...\"\n",
        "\n",
        "    max_length = num_sentences * 15\n",
        "    min_length = max(5, num_sentences * 5)\n",
        "\n",
        "    try:\n",
        "        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        return f\"Error during summarization: {e}\"\n",
        "\n",
        "def save_summary(summary_text):\n",
        "    if isinstance(summary_text, str) and len(summary_text.strip()) > 0:\n",
        "        filename = \"summary.txt\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(summary_text)\n",
        "        return filename\n",
        "    return None\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=summarize_text,\n",
        "    inputs=[\n",
        "        gr.File(file_types=[\".txt\", \".pdf\", \".docx\"], label=\"Upload a text, PDF, or Word (.docx) file\"),\n",
        "        gr.Slider(minimum=1, maximum=10, step=1, value=3, label=\"Number of sentences in summary\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Summary\"),\n",
        "    title=\"Multiformat Text Summarization Tool\",\n",
        "    description=\"Upload .txt, .pdf or .docx files and get an NLP-generated summary. Adjust summary length.\",\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "# Add a download button for the summary output\n",
        "download_btn = gr.File(label=\"Download Summary\", interactive=False)\n",
        "\n",
        "def summarize_and_return_file(file, num_sentences):\n",
        "    summary = summarize_text(file, num_sentences)\n",
        "    # Save summary to file for download\n",
        "    if isinstance(summary, str) and not summary.startswith(\"Error\") and not summary.startswith(\"Please\"):\n",
        "        filename = \"summary.txt\"\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(summary)\n",
        "        return summary, filename\n",
        "    else:\n",
        "        return summary, None\n",
        "\n",
        "iface2 = gr.Interface(\n",
        "    fn=summarize_and_return_file,\n",
        "    inputs=[\n",
        "        gr.File(file_types=[\".txt\", \".pdf\", \".docx\"], label=\"Upload a file\"),\n",
        "        gr.Slider(minimum=1, maximum=10, step=1, value=3, label=\"Number of sentences\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Summary\"),\n",
        "        gr.File(label=\"Download Summary\")\n",
        "    ],\n",
        "    title=\"Summarization with Download\",\n",
        "    description=\"Upload a .txt, .pdf or .docx file and get a summary with an option to download.\",\n",
        ")\n",
        "\n",
        "iface2.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "aFr6AfIcEUf3",
        "outputId": "26aa116a-2a26-40d9-fb71-d2d0b902fb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4979563ddc812ac339.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4979563ddc812ac339.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}